{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36cd104d",
   "metadata": {},
   "source": [
    "# CatBoost Model: Drift & Segmentation Analysis\n",
    "This notebook demonstrates how to train a CatBoost model, analyze drift, and perform segmentation analysis with interactive Plotly visualizations using the `tab-right` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bbf652c039a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if running in Colab or a fresh environment\n",
    "# !pip install catboost plotly pandas scikit-learn tab-right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be18540",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d9460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "pio.renderers.default = \"notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f859d1d3",
   "metadata": {},
   "source": [
    "## Load Example Dataset\n",
    "We'll use the UCI Adult dataset (census income) from OpenML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8de75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "data = fetch_openml(\"adult\", version=2, as_frame=True)\n",
    "df = data.frame.copy()\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n",
    "df = df.dropna()  # Drop missing for simplicity\n",
    "df[\"target\"] = (df[\"class\"] == \">50K\").astype(int)\n",
    "df = df.drop(columns=[\"class\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da731ac",
   "metadata": {},
   "source": [
    "## Split Data: Reference vs. Current\n",
    "We'll simulate drift by splitting the data by time (first 70% as reference, last 30% as current)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476604c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = int(0.7 * len(df))\n",
    "df_ref = df.iloc[:split_idx].reset_index(drop=True)\n",
    "df_cur = df.iloc[split_idx:].reset_index(drop=True)\n",
    "print(f\"Reference: {df_ref.shape}, Current: {df_cur.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebf81f2",
   "metadata": {},
   "source": [
    "## Train CatBoost Model\n",
    "We'll train on the reference data and predict on the current data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8750bd41572960e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = df_ref.select_dtypes(include=\"category\").columns.tolist() + [\n",
    "    col for col in df_ref.columns if df_ref[col].dtype == \"object\"\n",
    "]\n",
    "cat_features = list(set(cat_features) - set([\"target\"]))\n",
    "X_ref = df_ref.drop(columns=[\"target\"])\n",
    "y_ref = df_ref[\"target\"]\n",
    "X_cur = df_cur.drop(columns=[\"target\"])\n",
    "y_cur = df_cur[\"target\"]\n",
    "model = CatBoostClassifier(\n",
    "    cat_features=cat_features, iterations=50, depth=3, learning_rate=0.1, random_seed=42, verbose=0\n",
    ")\n",
    "model.fit(X_ref, y_ref)\n",
    "y_pred = model.predict(X_cur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9d808e",
   "metadata": {},
   "source": [
    "## Segmentation Analysis\n",
    "Let's segment the predictions by features and visualize the results using tab_right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b364694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules from tab_right\n",
    "import numpy as np\n",
    "\n",
    "from tab_right.plotting.plot_segmentations import DoubleSegmPlotting\n",
    "from tab_right.segmentations.double_seg import DoubleSegmentationImp\n",
    "from tab_right.segmentations.find_seg import FindSegmentationImp\n",
    "\n",
    "# Import specific modules from tab_right\n",
    "from tab_right.task_detection import detect_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509eb065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model predictions with probabilities\n",
    "y_pred_proba = model.predict_proba(X_cur)\n",
    "\n",
    "# Create a DataFrame with features, true labels, and predictions\n",
    "df_analysis = X_cur.copy()\n",
    "df_analysis[\"target\"] = y_cur\n",
    "df_analysis[\"pred_class\"] = y_pred\n",
    "df_analysis[\"pred_prob_0\"] = y_pred_proba[:, 0]\n",
    "df_analysis[\"pred_prob_1\"] = y_pred_proba[:, 1]\n",
    "\n",
    "# Detect task type (should be binary classification)\n",
    "task_type = detect_task(y_cur)\n",
    "print(f\"Detected task type: {task_type.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c9d34",
   "metadata": {},
   "source": [
    "### Define Error Metrics\n",
    "Let's define some error metrics for our segmentation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d9585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_log_loss(y_true, y_pred_df):\n",
    "    \"\"\"Calculate binary log loss for each row.\"\"\"\n",
    "    y_pred = y_pred_df[\"pred_prob_1\"]\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "\n",
    "def binary_error(y_true, y_pred_df):\n",
    "    \"\"\"Calculate binary classification error (0/1 loss) for each row.\"\"\"\n",
    "    y_pred = y_pred_df[\"pred_class\"]\n",
    "    return (y_true != y_pred).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460172d3",
   "metadata": {},
   "source": [
    "### Single Feature Segmentation\n",
    "Let's analyze how model performance varies across different segments of a single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76167423ce2d6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54b5ff3e45f3e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the segmentation finder\n",
    "segmentation_finder = FindSegmentationImp(\n",
    "    df=df_analysis, label_col=\"target\", prediction_col=[\"pred_prob_0\", \"pred_prob_1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa38932d",
   "metadata": {},
   "source": [
    "### Double Feature Segmentation\n",
    "Now let's analyze how model performance varies across segments defined by two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ba613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_func(y_true, y_pred):\n",
    "    \"\"\"Calculate mean log loss for a segment.\"\"\"\n",
    "    return (y_pred - y_true).abs()\n",
    "\n",
    "\n",
    "# Initialize the double segmentation\n",
    "double_segmentation = DoubleSegmentationImp(segmentation_finder)\n",
    "\n",
    "# Define feature pairs to analyze\n",
    "feature_pairs = [(\"age\", \"education-num\")]\n",
    "\n",
    "# Analyze each feature pair and visualize\n",
    "for feature1, feature2 in feature_pairs:\n",
    "    print(f\"\\nAnalyzing feature pair: {feature1} and {feature2}\")\n",
    "\n",
    "    # Find double segmentation\n",
    "    double_segments = double_segmentation(\n",
    "        feature1,\n",
    "        feature2,\n",
    "        error_func,\n",
    "        tree_model,\n",
    "        score_metric=log_loss,\n",
    "    )\n",
    "    print(f\"Found {len(double_segments)} segment combinations\")\n",
    "\n",
    "    # Display the segments\n",
    "    display(double_segments.head())\n",
    "\n",
    "    # Create double segmentation plotter using the default column name 'score'\n",
    "    double_plotter = DoubleSegmPlotting(df=double_segments)\n",
    "\n",
    "    # Plot the heatmap\n",
    "    heatmap_fig = double_plotter.plotly_heatmap()\n",
    "    heatmap_fig.update_layout(\n",
    "        title=f\"Log Loss Heatmap: {feature1} vs {feature2}\", xaxis_title=feature1, yaxis_title=feature2\n",
    "    )\n",
    "    heatmap_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae06725",
   "metadata": {},
   "source": [
    "## Performance Analysis by Features\n",
    "Let's take a deeper look at how the model performs across categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38940bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical features\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def safe_roc_auc_score(y_true, y_pred):\n",
    "    \"\"\"Calculate ROC AUC score with error handling for single-class data.\"\"\"\n",
    "    try:\n",
    "        if len(set(y_true)) < 2:\n",
    "            return None  # Not enough classes for ROC AUC\n",
    "        return roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Select some categorical features to analyze\n",
    "cat_features_to_analyze = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\"]\n",
    "\n",
    "for cat_feature in cat_features_to_analyze:\n",
    "    # Group by the categorical feature\n",
    "    grouped = df_analysis.groupby(cat_feature).agg({\n",
    "        \"target\": [\"count\", \"mean\"],\n",
    "        \"pred_class\": lambda x: accuracy_score(df_analysis.loc[x.index, \"target\"], x),\n",
    "        \"pred_prob_1\": lambda x: safe_roc_auc_score(df_analysis.loc[x.index, \"target\"], x),\n",
    "    })\n",
    "\n",
    "    # Flatten the column hierarchy\n",
    "    grouped.columns = [f\"{col[0]}_{col[1]}\" if col[1] else col[0] for col in grouped.columns]\n",
    "    grouped = grouped.rename(columns={\"pred_class_<lambda>\": \"accuracy\", \"pred_prob_1_<lambda>\": \"auc\"})\n",
    "    grouped = grouped.reset_index()\n",
    "\n",
    "    # Filter out categories with no AUC score for better visualization\n",
    "    grouped_filtered = grouped[grouped[\"auc\"].notna()]\n",
    "\n",
    "    # Plot accuracy by category\n",
    "    if not grouped_filtered.empty:\n",
    "        fig = px.bar(\n",
    "            grouped_filtered,\n",
    "            x=cat_feature,\n",
    "            y=\"accuracy\",\n",
    "            color=\"target_count\",\n",
    "            hover_data=[\"target_mean\", \"auc\", \"target_count\"],\n",
    "            labels={\n",
    "                \"accuracy\": \"Accuracy\",\n",
    "                \"target_count\": \"Sample Count\",\n",
    "                \"target_mean\": \"Positive Rate\",\n",
    "                \"auc\": \"AUC\",\n",
    "            },\n",
    "            title=f\"Model Performance by {cat_feature}\",\n",
    "        )\n",
    "        fig.update_layout(xaxis_tickangle=-45)\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fcb4d8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
